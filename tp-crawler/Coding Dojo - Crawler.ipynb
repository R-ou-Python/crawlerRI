{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ☯ Coding Dojo ☯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:left\" src=\"imgs/vamos_coletar.png\">\n",
    "## Regras\n",
    "\n",
    "Ponto de participação, poderá perdido se:\n",
    "\n",
    "    - Aluno atrasado (ver política de atraso na especificação)\n",
    "    - Recusar participação como piloto/copiloto\n",
    "        - Ou não querer sair do computador, quando solicitado :-)\n",
    "    - Demorar para sair quando solicitado\n",
    "    - Parar de participar por:\n",
    "        - Uso de outro computador\n",
    "        - Uso de celular\n",
    "        - Usar a internet\n",
    "        - Conversa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de entregar, certifique-se que tenha executado todos os comandos/códigos deste Jupyter. É obrigatório que todas as saídas tenham sido apresentadas. Perda de 3 pontos caso não tenham feito isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se necessário, instale o python3 e o pip3 abaixo usando `sudo apt-get install python3 python3-pip`.  Execute abaixo para instalar o [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/). Qualquer linha de comando/código como a de baixo, você pode executá-la a selecionando e pressionando `ctrl+enter` ❣️."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /home/mari/anaconda3/lib/python3.7/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/mari/anaconda3/lib/python3.7/site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/mari/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->bs4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso de comando não encontrado `pip3` use `pip`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coletor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/arquitetura_coletor.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você deverá completar a implementação das três classes abaixo para fazer o coletor: `Domain`, `Scheduler` e `PageFetcher`. Conforme figura acima, o `Scheduler` é responsável por armazenar as filas de URLs a serem requisitadas; o PageFetcher serão *threads* responsáveis por fazer as requisições das URLs obtidas por meio do escalonador (instancia da classe `Scheduler`). A classe `Domain` armazena informações importantes sobre o servidor a serem usadas no momento do escalonamento das URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe `Domain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"imgs/estrutura_coletor.png\">\n",
    "    <caption>Fonte: Baeza-Yates e Ribeiro-Neto, 2011 </caption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de começar a fazer o escalonador (classe `Scheduler`) devemos implementar a classe que representa os domínios. Conforme visto na figura acima, o escalonador possuirá diversas filas, uma para cada servidor.  Em nosso caso, o servidor será um domínio - que você irá implementar na classe `Domain` do arquivo `domain.py`. \n",
    "\n",
    "O escalonador percorrerá cada servidor e obterá o primeiro da fila do primeiro servidor acessível. O servidor é acessível obdecendo o limite de tempo entre requisições (em segundos, usando o atributo `int_time_limit_between_requests`). \n",
    "\n",
    "Nesta atividade você irá implementar a classe `Domain`, que já possui os seguintes atributos: \n",
    "\n",
    "- *int_time_limit_between_requests*: Limite entre requisições (int)\n",
    "- *nam_domain*: Nome do domínio (String)\n",
    "- *time_last_access*: [Objeto Datetime](https://docs.python.org/3/library/datetime.html#datetime-objects) informando da data/hora do último acesso \n",
    "\n",
    "O prefixo da variável sempre irá denotar o tipo da mesma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 1 - classe `Domain`**: Complete a classe `Domain` implementando os métodos/atributos calculados descritos a seguir. Logo após, execute o ❣️ teste unitário ❣️ para verificar se os métodos/atributos calculados foram implementados conforme esperado.\n",
    "\n",
    "- **accessed_now**: Método que modifica o último acesso com a data/hora atual usando um [objeto datetime](https://docs.python.org/3/library/datetime.html#datetime-objects)\n",
    "- **time_since_last_access**: Atributo calculado que retorna um [objeto TimeDelta](https://docs.python.org/3/library/datetime.html#timedelta-objects) com a diferença da data atual e a data do último acesso. Veja os exemplos de uso do TimeDelta na sua documentação\n",
    "- **is_accessible**: Método que verdadeiro se o domínio estiver acessível"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os métodos `count_fetched_page` e `has_finished_crawl` já estão implementados. Execute o teste unitário a seguir. Veja que o comando abaixo executa o teste unitário presente no arquivo `crawler/scheduler_test.py`. Analise-o para entender como implementamos testes unitários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando acesso a um dominio já requisitado (após espera)\n",
      "aguardando 10 segundos...\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 10.008s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m crawler.scheduler_test DomainTest.test_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 2 - métodos `__hash__` e `__eq__` da classe `Domain`**: A fila será implementada por um dicionario ordenado ([OrderedDict](https://docs.python.org/2/library/collections.html#collections.OrderedDict)) em que a chave será um objeto da classe domínio e o valor serão uma lista de URLs. Para que seja possível a busca do domínio, você deverá implementar os métodos `__hash__` e `__eq__` de forma similar ao seguinte exemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "class Xuxu():\n",
    "    def __str__(self):\n",
    "        return \"xuxuuu\"\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(\"xuxu\")\n",
    "    def __eq__(self, x):\n",
    "        return x == \"xuxu\"\n",
    "d = OrderedDict()\n",
    "d[Xuxu()] = 'A'\n",
    "print(\"xuxu\" in d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso caso, deve ser permitido fazer a busca no dicionario pelo nome do domínio ou pelo objeto, ou seja, após a implementação desta atividade, o seguinte código irá funcionar corretamente: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oi\n",
      "oi\n"
     ]
    }
   ],
   "source": [
    "from crawler.domain import Domain\n",
    "from collections import OrderedDict\n",
    "\n",
    "obj_domain = Domain(\"oi.com\",10)\n",
    "dic_x = OrderedDict()\n",
    "dic_x[obj_domain] = \"lala\"\n",
    "dic_x[\"oi.com\"] = \"oi\"\n",
    "\n",
    "#abaixo, em ambos os casos, será encontrado/impresso a string \"oi\"\n",
    "print(dic_x[\"oi.com\"])\n",
    "print(dic_x[obj_domain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se que, em nosso caso, o parametro do método `__eq__` pode ser um objeto da classe `Domain` ou uma string representando o nome do domínio. Caso faça alguma modificação no código, você deverá reiniciar o kernel para que o efeito seja visto no código acima (para reiniciar, clique em Kernel->Restart). Isso ocorre que, uma vez dado import em um módulo, esse modulo não é alterado no Jupyter caso você faça uma modificação no código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe `Scheduler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora você irá implementar o escalonador. Essa classe será responsável por gerenciar as URLs. \n",
    "Para isso, será implementado as filas por servidores conforme apresentado na figura da seção anterior. \n",
    "Para isso, utilizaremos um OrderedDict em que as chaves são os servidores (objeto da classe `Domain`) e o valor será uma lista de tuplas com as URLs e sua profundidade. Com o objetivo de armazenar as URLs de forma estruturada, as mesmas **não serão** string, ao invés disso, serão objetos da classe `ParseResult` - essa classe é retornada do método [urlparse](https://docs.python.org/3/library/urllib.parse.html). Veja um exemplo abaixo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crawler.domain import Domain\n",
    "from crawler.scheduler import Scheduler\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "dict_filas_por_servidor = {Domain(\"www.globo.com\",10):[(urlparse(\"http://www.globo.com/esporte\"),1),\n",
    "                                                (urlparse(\"http://www.globo.com/noticia_um\"),2),\n",
    "                                                (urlparse(\"http://www.globo.com/noticia_dois\"),3),\n",
    "                                                (urlparse(\"http://www.globo.com/noticia_tres\"),3),\n",
    "                                                ],\n",
    "                            Domain(\"www.cnpq.br\",10):[(urlparse(\"http://www.cnpq.br/pesquisadores\"),1),\n",
    "                                               (urlparse(\"http://www.cnpq.br/bolsas\"),1)]\n",
    "                          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, a classe `Scheduler`, no arquivo `scheduler.py` possui os seguintes atributos: \n",
    "\n",
    "- `str_usr_agent`: Nome do `User agent`. Usualmente, é o nome do navegador, em nosso caso,  será o nome do coletor (usualmente, terminado em `bot`)\n",
    "- `int_page_limit`: Número de páginas a serem coletadas\n",
    "- `int_depth_limit`: Profundidade máxima a ser coletada\n",
    "- `int_page_count`: Quantidade de página já coletada\n",
    "- `dic_url_per_domain`: Fila de URLs por domínio (explicado anteriormente)\n",
    "- `set_discovered_urls`: Conjunto de URLs descobertas, ou seja, que foi extraída em algum HTML e já adicionadas na fila - mesmo se já ela foi retirada da fila. A URL armazenada deve ser uma string.\n",
    "- `dic_robots_per_domain`: Dicionário armazenando, para cada domínio, o objeto representando as regras obtidas no `robots.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividad 3 - Método `can_add_page`**: Esse método irá retornar verdadeiro caso seja possível adicionar a página `obj_url` (objeto da classe ParseResult) que foi coletada na profundidade `int_depth`. Para que seja possível adicionar na lista, esta pagina não deve ter sido descoberta e nem com a profundidade maior que o limite. Este método será testado por teste unitário apenas após a implementação da adição e obtenção das URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 4 método `add_new_page` - Adicionar nova página**:  Este método retorna falso caso não seja possível adicionar a página (por meio do método `can_add_page`) e verdadeiro, caso contrário. Caso seja possível, deve-se adicionar a url `obj_url` na fila `dic_url_per_domain`. Lembre-se da estrutura do dicionário `dic_url_per_domain`. Não esqueça de armazenar que esta URL já foi descoberta. Este método será testado por teste unitário apenas após a implementação da obtenção das URLs. Lembre-se de como trabalhar com [dicionários](https://daniel-hasan.github.io/cefet-web-grad/classes/python2/#mais-colecoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5 método `get_next_url` - obtém a próxima URL da fila**: Este método retona uma tupla `url,profundade` da próxima URL a ser coletada por meio da fila `dic_url_per_domain` retirando este item da fila. Lembre-se que: \n",
    "    \n",
    "- Ele deverá retornar a primeira URL do primeiro servidor que estiver acessível. \n",
    "- Você deve indicar que o servidor foi acessado \n",
    "- Caso a fila deste servidor esteja vazia, elimine-o do dicionário\n",
    "- Caso não encontre a URL, [coloque a Thread para esperar](https://docs.python.org/2/library/time.html#time.sleep) e, logo após, procure novamente \n",
    "\n",
    "Logo após, você já pode executar o teste unitário para verificar se a obtenção e adição de URLs está correta. Caso haja algum problema, existem já alguns prints comentados no código, descomente-os para verificar se o fluxo está ocorrendo da forma correta. Você deverá entregar sem a exibição desses prints (caso contrário, perderá pontos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificação da ordem das URLs...\n",
      "Resgatando a segunda página do mesmo dominio...\n",
      "Tempo esperado: 20 segundos\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 20.020s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m crawler.scheduler_test SchedulerTest.test_add_remove_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 6 método `can_fetch_page`**: Este método deve retornar verdadeiro caso a url (Objeto `ParseResult`), passada como parâmetro, pode ser coletada de acordo com o `Robots.txt` do domínio. Para isso, você deve usar o [`RobotFileParser`](https://docs.python.org/3/library/urllib.robotparser.html). Por razões de performance, você só poderá requisitar o robots.txt uma única vez por domínio. Assim, use o `dic_robots_per_domain` apropriadamente. Execute o teste unitário abaixo para verificar seu funcionamento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 131.751s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m crawler.scheduler_test SchedulerTest.test_can_fetch_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 7 - Inicialização das sementes no contrutor**: No contrutor da classe `Scheduler` você deve adicionar as páginas sementes na fila. Tais sementes são passadas pelo parametro `arr_urls_seeds` do contrutor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m crawler.scheduler_test SchedulerTest.test_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo após, utize a anotação @synchronized aonde julgar necessário para transformar em ThreadSafe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageFetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 8 - Efetuar a requisição - método `request_url`:** Neste método você irá fazer a requisição usando a api [requests](https://requests.readthedocs.io/en/master/). Além disso, deve-se informar no cabeçalho (em `User-Agent`) o nome do coletor. O método deverá retornar o conteúdo (em binário) apenas se o conteúdo for HTML (ver no cabeçalho da resposta o tipo do conteúdo). Caso não seja, ele deverá retornar nulo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.926s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m crawler.page_fetcher_test PageFetcherTest.test_request_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 9 extração das URLs por meio do método `discover_links`**: A partir do conteúdo (texto em binário) `bin_str_content` deve-se extrair seus os links. O conteúdo esta em binário, pois, o BeautifulSoup, internamente, verifica o encoding por meio da [tag meta](https://www.w3schools.com/tags/att_meta_charset.asp) e converte para UTF8. A URL da página coletada é um objeto ParseResult `obj_url` que está na profundidade `int_depth`. \n",
    "\n",
    "Para cada URL extraída, você deverá retornar uma tupla com a URL (objeto ParseResult) e a sua profundidade. Utilize o comando `yield` para retornar essas tuplas. A profundidade é calculada de por meio de  `obj_url` e `int_depth`, conforme comentado em sala de aula. \n",
    "\n",
    "Para a extração dos links, use o [CSS Selector do BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors). Seletor CSS é uma expressão que define quais elementos HTML serão estilizados pelo CSS. No *Beatifulsoup* eles são usados para selecionar quais tags serão extraídas. Veja o exemplo de uso e brinque abaixo. Caso precise, veja aqui um pouco de seletores CSS: [slides de Web](https://fegemo.github.io/cefet-front-end/classes/css2/#outros-seletores), [documentação MDN](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulação da extração de links da página http://www.pudim.com.br na profundidade nível 2...\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.063s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m crawler.page_fetcher_test PageFetcherTest.test_discover_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 10 - método que solicita uma nova URL**: Por meio do método `crawl_new_url` você deverá utilizar os métodos previamente implementados para:\n",
    "\n",
    "- Solicitar ao escalonador uma nova URL\n",
    "- Fazer a requisição e obter o resultado (em binário)\n",
    "- Caso a URL seja um HTML válido, imprima esta URL e extraia os seus links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 12 - método run**: Este método deve coletar páginas enquanto a coleta não foi finalizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 11 - um pequeno teste para finalizar: ** Use as sementes do seu grupo e crie abaixo um escalonador e 5 PageFetchers para extrair 30 páginas. Imprima também o tempo gasto total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='https', netloc='www.cefetmg.br', path='', params='', query='', fragment='')\n",
      "https://www.cefetmg.br\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno -2] Name or service not known>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1319\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1320\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    937\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 938\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    939\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a2dadf9bd52d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msorvete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPageFetcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempadinha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcoquinhagelada\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mpaodequeijo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcoxinha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Recuperacao_Informacao/crawlerRI/tp-crawler/crawler/page_fetcher.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_finished_crawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# Pega todos os conjuntos de tuplas da crawl (retorno da discovery_links)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl_new_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_fetched_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Recuperacao_Informacao/crawlerRI/tp-crawler/crawler/page_fetcher.py\u001b[0m in \u001b[0;36mcrawl_new_url\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0murl_returned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_fetch_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_returned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Recuperacao_Informacao/crawlerRI/tp-crawler/crawler/scheduler.py\u001b[0m in \u001b[0;36mcan_fetch_page\u001b[0;34m(self, obj_url)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mrobotsTxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobj_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/robots.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mrobotFileParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrobotsTxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mrobotFileParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdic_robots_per_domain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetloc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrobotFileParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/robotparser.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;34m\"\"\"Reads the robots.txt URL and feeds it to the parser.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m401\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 543\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0mhttp_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1320\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -2] Name or service not known>"
     ]
    }
   ],
   "source": [
    "from crawler.scheduler import Scheduler\n",
    "from crawler.page_fetcher import PageFetcher\n",
    "\n",
    "arr_urls_seeds = [\"http://www.crea-mg.org.br\",\"https://www.cefetmg.br\",\"https://youtube.com\"]\n",
    "empadinha = Scheduler(str_usr_agent=\"crawlbot\", int_page_limit=30, int_depth_limit=3, arr_urls_seeds=arr_urls_seeds)\n",
    "\n",
    "coquinhagelada = PageFetcher(empadinha)\n",
    "paodequeijo = PageFetcher(empadinha)\n",
    "coxinha = PageFetcher(empadinha)\n",
    "batatafrita = PageFetcher(empadinha)\n",
    "sorvete = PageFetcher(empadinha)\n",
    "\n",
    "coquinhagelada.run()\n",
    "paodequeijo.run()\n",
    "coxinha.run()\n",
    "batatafrita.run()\n",
    "sorvete.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografia\n",
    "\n",
    "Baeza-Yates, Ricardo; Ribeiro-Neto, Berthier. **Modern information retrieval: the concepts and technology behind search**. ACM Press, 2011.\n",
    "\n",
    "Batista, Natércia ; Brandão, Michele ; Pinheiro, Michele ; Dalip, Daniel ; Moro, Mirella . **[Dados de Múltiplas Fontes da Web: coleta, integração e pré-processamento](https://sol.sbc.org.br/livros/index.php/sbc/catalog/download/8/19/58-1?inline=1)**. Minicursos do XXIV Simpósio Brasileiro de Sistemas Multimídia e Web. 1ed.: Sociedade Brasileira de Computação, 2018, v. , p. 153-192.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
